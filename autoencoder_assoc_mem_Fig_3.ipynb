{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "pAZmzYxRqmcu"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm as tqdm\n",
    "    \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvdgX_yJG39p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "q43bh4q88OpI"
   },
   "outputs": [],
   "source": [
    "##This cell loads minimagenet with colored images\n",
    "\n",
    "c_miniimagenet = np.load(\"miniimagenet_3600_84_84_3.npy\")\n",
    "sbook =c_miniimagenet.reshape((3600,84*84*3))\n",
    "sbook_mean = np.mean(sbook.flatten())\n",
    "sbook = sbook - sbook_mean\n",
    "indices = np.concatenate([np.arange(i, 3600, 600) for i in range(600)])\n",
    "sbook = sbook[indices]\n",
    "data_features = sbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8f1svsrXb5YZ"
   },
   "outputs": [],
   "source": [
    "#This cell loads BW miniimagenet as predominantly used in the paper\n",
    "\n",
    "sbook = np.load('BW_miniimagenet_3600_60_60_full_rank.npy').reshape((3600,3600))\n",
    "indices = np.concatenate([np.arange(i, 3600, 600) for i in range(600)])\n",
    "sbook = sbook[indices]\n",
    "sbook = sbook - np.mean(sbook.flatten())\n",
    "data_features=sbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HXzYJ9y-vaiz"
   },
   "outputs": [],
   "source": [
    "def corrupt_p(codebook,p=0.1,booktype='01'):\n",
    "  if p==0.:\n",
    "    return codebook\n",
    "  rand_indices = np.sign(np.random.uniform(size=codebook.shape)- p )\n",
    "  if booktype=='-11':\n",
    "    return np.multiply(codebook,rand_indices)\n",
    "  elif booktype=='01':\n",
    "    return abs(codebook - 0.5*(-rand_indices+1))\n",
    "  elif booktype=='cts':\n",
    "    return codebook + np.random.normal(0,1,size=codebook.shape)*p\n",
    "  else:\n",
    "    print(\"codebook should be -11; 01; or cts\")\n",
    "    return 0\n",
    "\n",
    "def get_overlap(x,y,return_all=False,normalized=True):\n",
    "  if x.shape != y.shape:\n",
    "    return \"error\"\n",
    "  if normalized:\n",
    "    x = (x/np.linalg.norm(x,axis=1)[:,None])\n",
    "    y = (y/np.linalg.norm(y,axis=1)[:,None])\n",
    "    if return_all:\n",
    "      return np.einsum('ij,ij->i',x,y)\n",
    "    else:\n",
    "      return np.average(np.einsum('ij,ij->i',x,y))\n",
    "  else:\n",
    "    if return_all:\n",
    "      np.einsum('ij,ij->i',x,y)\n",
    "    else:\n",
    "      return np.average(np.einsum('ij,ij->i',x,y))\n",
    "\n",
    "def cleanup(s, sbook):\n",
    "  idx = np.argmax(sbook@s)\n",
    "  sclean = sbook[idx,:]\n",
    "  return sclean\n",
    "\n",
    "def binarize(data,bin_type='01'):\n",
    "  if bin_type=='01':\n",
    "    return 0.5*(np.sign(data - 0.5)+1)\n",
    "  elif bin_type=='-11':\n",
    "    return np.sign(data)\n",
    "\n",
    "def get_error(x,y):\n",
    "  if x.shape != y.shape:\n",
    "    return \"error\"\n",
    "  return np.sum(abs(x-y))/np.prod(x.shape)\n",
    "\n",
    "def get_mse(x,y):\n",
    "  if x.shape != y.shape:\n",
    "    return \"error\"\n",
    "  return np.average((x-y)**2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1LGpuUQmSgu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0itSdsqItHHa",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Varying Npatts binary sbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tf4sHb4Zc0GB",
    "outputId": "65bd32e1-2982-42b4-8354-eaf7152e60aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [08:55<00:00, 66.96s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Npatts_list = np.arange(1,3601,500)\n",
    "nruns = 1\n",
    "first_clean_overlap_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "final_clean_overlap_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "\n",
    "first_noisy_overlap_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "final_noisy_overlap_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "\n",
    "first_noisy_p_overlap_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "final_noisy_p_overlap_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "\n",
    "verbose=False\n",
    "\n",
    "for nidx in (range(nruns)):\n",
    "  for idx,Npatts in enumerate(tqdm(Npatts_list)):\n",
    "    if verbose:\n",
    "      print(\"xxxxx nidx = \"+str(nidx))\n",
    "      print(\"xxxxx idx = \"+str(idx))\n",
    "      print(\"xxxxx Npatts = \"+str(Npatts))\n",
    "    Ng=9+16+25\n",
    "    Np=400\n",
    "    Ns=3600\n",
    "    # Npatts=200\n",
    "    data_shape = Ns\n",
    "    test_noise_frac=0.0\n",
    "    test_noise_frac_s=0.1\n",
    "    test_noise_frac_p=1.0\n",
    "\n",
    "    sbook = np.sign(np.random.uniform(size=(Npatts,Ns))-0.5)\n",
    "    \n",
    "    sperm = sbook[np.random.permutation(Npatts)]\n",
    "    true_data = sperm[:Npatts]   \n",
    "\n",
    "    #Autoencoder model below defined with encoder as being s>p and decoder as p>g>p>s so that noise injected after encoder is p noise\n",
    "    class Autoencoder(Model):\n",
    "      def __init__(self, Np,Ng,Ns):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.Np = Np\n",
    "        self.Ns = Ns\n",
    "        self.Ng = Ng\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "          layers.Dense(Np, activation='tanh'),\n",
    "          # layers.Dense(Ng, activation='tanh'),\n",
    "        ])\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "          layers.Dense(Ng, activation='tanh'),\n",
    "          layers.Dense(Np, activation='tanh'),\n",
    "          layers.Dense(Ns, activation='tanh')\n",
    "        ])\n",
    "\n",
    "      def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    autoencoder = Autoencoder(Np,Ng,Ns)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\n",
    "    autoencoder.fit(true_data, true_data, epochs=100, verbose=0);\n",
    "    \n",
    "\n",
    "    if verbose: print(\"-------------Testing--1----------------\")\n",
    "    if verbose: print(\"Testing noise percentage = \"+str(test_noise_frac))\n",
    "    cts_corrupt = corrupt_p(true_data,p=0,booktype='cts')\n",
    "\n",
    "    encoded_data = autoencoder.encoder(cts_corrupt[:Npatts]).numpy()\n",
    "    decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "    decoded_data1 = binarize(np.copy(decoded_data),bin_type='-11')\n",
    "    # decoded_data1 = np.copy(decoded_data)\n",
    "    iters=50\n",
    "    for _ in range(iters):\n",
    "        encoded_data = autoencoder.encoder(decoded_data).numpy()\n",
    "        decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "    decoded_data = binarize(decoded_data,bin_type='-11')\n",
    "    mean_p_val = np.average(np.linalg.norm(encoded_data,axis=1))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Npatts = \"+str(Npatts))\n",
    "        # print(\"Initial error: \" + str(get_mse(true_data[:,:data_shape], cts_corrupt[:Npatts,:data_shape])))\n",
    "        # print(\"error after one timestep: \" + str(get_mse(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])))\n",
    "        # print(\"error after iteration: \" + str(get_mse(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])))\n",
    "\n",
    "        print(\"overlap after one timestep: \" + str(get_overlap(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])))\n",
    "        print(\"overlap after iteration: \" + str(get_overlap(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])))\n",
    "\n",
    "    first_clean_overlap_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])\n",
    "    final_clean_overlap_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])\n",
    "\n",
    "    \n",
    "    if verbose: print(\"-------------Testing--2----------------\")\n",
    "    if verbose: print(\"Testing noise percentage = \"+str(test_noise_frac_s))\n",
    "    \n",
    "    cts_corrupt = corrupt_p(true_data,p=test_noise_frac_s,booktype='-11')\n",
    "\n",
    "    encoded_data = autoencoder.encoder(cts_corrupt[:Npatts]).numpy()\n",
    "    decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "    decoded_data1 = binarize(np.copy(decoded_data),bin_type='-11')\n",
    "    # decoded_data1 = np.copy(decoded_data)\n",
    "    iters=50\n",
    "    for _ in range(iters):\n",
    "      encoded_data = autoencoder.encoder(decoded_data).numpy()\n",
    "      decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "    decoded_data = binarize(decoded_data,bin_type='-11')\n",
    "\n",
    "    if verbose:\n",
    "      print(\"Np = \"+str(Np))\n",
    "      print(\"Ns = \"+str(Ns))\n",
    "      print(\"Ng = \"+str(Ng))\n",
    "      print(\"Npatts = \"+str(Npatts))\n",
    "\n",
    "\n",
    "      print(\"Initial overlap: \" + str(get_overlap(true_data[:,:data_shape], cts_corrupt[:Npatts,:data_shape])))\n",
    "      print(\"overlap after one timestep: \" + str(get_overlap(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])))\n",
    "      print(\"overlap after iteration: \" + str(get_overlap(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])))\n",
    "\n",
    "    first_noisy_overlap_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])\n",
    "    final_noisy_overlap_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])\n",
    "\n",
    "    \n",
    "    if verbose: print(\"-------------Testing--3----------------\")\n",
    "    if verbose: print(\"Testing noise percentage = \"+str(test_noise_frac_p))\n",
    "    cts_corrupt = corrupt_p(true_data,p=0,booktype='cts')\n",
    "\n",
    "    encoded_data = autoencoder.encoder(cts_corrupt[:Npatts]).numpy()\n",
    "    encoded_data = encoded_data + test_noise_frac_p*mean_p_val*np.random.normal(0,1,encoded_data.shape)/np.sqrt(Np)\n",
    "    decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "    decoded_data1 = binarize(np.copy(decoded_data),bin_type='-11')\n",
    "    # decoded_data1 = np.copy(decoded_data)\n",
    "    iters=50\n",
    "    for _ in range(iters):\n",
    "      encoded_data = autoencoder.encoder(decoded_data).numpy()\n",
    "      decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "    decoded_data = binarize(decoded_data,bin_type='-11')\n",
    "\n",
    "    if verbose:\n",
    "      print(\"Np = \"+str(Np))\n",
    "      print(\"Ns = \"+str(Ns))\n",
    "      print(\"Ng = \"+str(Ng))\n",
    "      print(\"Npatts = \"+str(Npatts))\n",
    "\n",
    "\n",
    "      print(\"Initial error: \" + str(get_mse(true_data[:,:data_shape], cts_corrupt[:Npatts,:data_shape])))\n",
    "      print(\"error after one timestep: \" + str(get_mse(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])))\n",
    "      print(\"error after iteration: \" + str(get_mse(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])))\n",
    "\n",
    "      print(\"Initial overlap: \" + str(get_overlap(true_data[:,:data_shape], cts_corrupt[:Npatts,:data_shape])))\n",
    "      print(\"overlap after one timestep: \" + str(get_overlap(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])))\n",
    "      print(\"overlap after iteration: \" + str(get_overlap(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])))\n",
    "\n",
    "    first_noisy_p_overlap_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])\n",
    "    final_noisy_p_overlap_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Varying Npatts BW miniimagenet sbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 16625 calls to <function TensorFlowTrainer._make_function.<locals>.multi_step_on_iterator at 0x000001D911DFAD30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x000001D8C78DE5E0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\uvvenv\\bioinfo\\lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"d:\\uvvenv\\bioinfo\\lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n",
      "  8%|▊         | 6/72 [00:12<02:16,  2.07s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5724\\3966386936.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mautoencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMeanSquaredError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\uvvenv\\bioinfo\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\uvvenv\\bioinfo\\lib\\site-packages\\keras\\src\\layers\\layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    970\u001b[0m                     \u001b[1;34m\"layers will not see the mask.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    971\u001b[0m                 )\n\u001b[0;32m    972\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    973\u001b[0m             \u001b[1;31m# Destroy call context if we created it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 974\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_reset_call_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    975\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m         \u001b[1;31m################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m         \u001b[1;31m# 8. Add a node in the graph for symbolic calls.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\uvvenv\\bioinfo\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\uvvenv\\bioinfo\\lib\\site-packages\\keras\\src\\ops\\operation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[0;32m     55\u001b[0m                 \u001b[0mcall_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mobject_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.__class__.__name__}.call()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             )\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# Plain flow.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\uvvenv\\bioinfo\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\uvvenv\\bioinfo\\lib\\site-packages\\keras\\src\\models\\sequential.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, inputs, training, mask, **kwargs)\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_functional\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m             return self._functional.call(\n\u001b[0m\u001b[0;32m    221\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             )\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\uvvenv\\bioinfo\\lib\\site-packages\\keras\\src\\models\\functional.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, inputs, training, mask, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[0mmasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_keras_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         outputs = self._run_through_graph(\n\u001b[0m\u001b[0;32m    184\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m             operation_fn=lambda op: operation_fn(\n\u001b[0;32m    186\u001b[0m                 \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\uvvenv\\bioinfo\\lib\\site-packages\\keras\\src\\ops\\function.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, inputs, operation_fn, call_fn)\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moperation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcall_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[1;31m# Update tensor_dict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\uvvenv\\bioinfo\\lib\\site-packages\\keras\\src\\models\\functional.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[1;32mand\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m             ):\n\u001b[0;32m    646\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0moperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\uvvenv\\bioinfo\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\uvvenv\\bioinfo\\lib\\site-packages\\keras\\src\\layers\\layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    970\u001b[0m                     \u001b[1;34m\"layers will not see the mask.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    971\u001b[0m                 )\n\u001b[0;32m    972\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    973\u001b[0m             \u001b[1;31m# Destroy call context if we created it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 974\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_reset_call_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    975\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m         \u001b[1;31m################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m         \u001b[1;31m# 8. Add a node in the graph for symbolic calls.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\uvvenv\\bioinfo\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\uvvenv\\bioinfo\\lib\\site-packages\\keras\\src\\ops\\operation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[0;32m     55\u001b[0m                 \u001b[0mcall_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mobject_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.__class__.__name__}.call()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             )\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# Plain flow.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\uvvenv\\bioinfo\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\uvvenv\\bioinfo\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\uvvenv\\bioinfo\\lib\\site-packages\\keras\\src\\ops\\numpy.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x1, x2)\u001b[0m\n\u001b[0;32m   4017\u001b[0m         \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0mproduct\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4018\u001b[0m     \"\"\"\n\u001b[0;32m   4019\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4020\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mMatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msymbolic_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4021\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\uvvenv\\bioinfo\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x1, x2)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx1_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx2_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mx2_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mx1_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\uvvenv\\bioinfo\\lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\uvvenv\\bioinfo\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\uvvenv\\bioinfo\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1268\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mOpDispatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNOT_SUPPORTED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1270\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1272\u001b[1;33m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\uvvenv\\bioinfo\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, grad_a, grad_b, name)\u001b[0m\n\u001b[0;32m   3696\u001b[0m             \u001b[0mgrad_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrad_b\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3697\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3698\u001b[0m         )\n\u001b[0;32m   3699\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3700\u001b[1;33m         return gen_math_ops.mat_mul(\n\u001b[0m\u001b[0;32m   3701\u001b[0m             \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3702\u001b[0m             \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3703\u001b[0m             \u001b[0mtranspose_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\uvvenv\\bioinfo\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(a, b, transpose_a, transpose_b, grad_a, grad_b, name)\u001b[0m\n\u001b[0;32m   6233\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6234\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6235\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6236\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6237\u001b[1;33m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6238\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6239\u001b[0m       return mat_mul_eager_fallback(\n\u001b[0;32m   6240\u001b[0m           \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "Npatts_list = np.arange(1,3601,50)\n",
    "nruns = 1\n",
    "first_clean_overlap_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "final_clean_overlap_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "\n",
    "first_noisy_overlap_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "final_noisy_overlap_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "\n",
    "first_noisy_p_overlap_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "final_noisy_p_overlap_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "\n",
    "verbose=True\n",
    "\n",
    "for nidx in (range(nruns)):\n",
    "  for idx,Npatts in enumerate(tqdm(Npatts_list)):\n",
    "    if verbose:\n",
    "      print(\"xxxxx nidx = \"+str(nidx))\n",
    "      print(\"xxxxx idx = \"+str(idx))\n",
    "      print(\"xxxxx Npatts = \"+str(Npatts))\n",
    "    Ng=9+16+25\n",
    "    Np=400\n",
    "    Ns=3600#84*84*3   ### Adjust accordingly if using colored images or BW images\n",
    "    # Npatts=200\n",
    "    data_shape = Ns\n",
    "    test_noise_frac=0.0\n",
    "    test_noise_frac_s=0.1\n",
    "    test_noise_frac_p=1.0\n",
    "\n",
    "    sbook = data_features - np.mean(data_features.flatten())\n",
    "\n",
    "    sperm = sbook[np.random.permutation(Npatts)]\n",
    "    true_data = sperm[:Npatts]   \n",
    "\n",
    "    #Autoencoder model below defined with encoder as being s>p and decoder as p>g>p>s so that noise injected after encoder is p noise\n",
    "    class Autoencoder(Model):\n",
    "      def __init__(self, Np,Ng,Ns):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.Np = Np\n",
    "        self.Ns = Ns\n",
    "        self.Ng = Ng\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "          layers.Dense(Np, activation='tanh'),\n",
    "          # layers.Dense(Ng, activation='tanh'),\n",
    "        ])\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "          layers.Dense(Ng, activation='tanh'),\n",
    "          layers.Dense(Np, activation='tanh'),\n",
    "          layers.Dense(Ns, activation='tanh')\n",
    "        ])\n",
    "\n",
    "      def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    autoencoder = Autoencoder(Np,Ng,Ns)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\n",
    "    autoencoder.fit(true_data, true_data, epochs=1, verbose=0);\n",
    "    \n",
    "\n",
    "    if verbose: print(\"-------------Testing--1----------------\")\n",
    "    if verbose: print(\"Testing noise percentage = \"+str(test_noise_frac))\n",
    "    cts_corrupt = corrupt_p(true_data,p=0,booktype='cts')\n",
    "\n",
    "    encoded_data = autoencoder.encoder(cts_corrupt[:Npatts]).numpy()\n",
    "    decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "    decoded_data1 = np.copy(decoded_data)\n",
    "    iters=50\n",
    "    for _ in range(iters):\n",
    "        encoded_data = autoencoder.encoder(decoded_data).numpy()\n",
    "        decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "    mean_p_val = np.average(np.linalg.norm(encoded_data,axis=1))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Npatts = \"+str(Npatts))\n",
    "        # print(\"Initial error: \" + str(get_mse(true_data[:,:data_shape], cts_corrupt[:Npatts,:data_shape])))\n",
    "        # print(\"error after one timestep: \" + str(get_mse(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])))\n",
    "        # print(\"error after iteration: \" + str(get_mse(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])))\n",
    "\n",
    "        print(\"overlap after one timestep: \" + str(get_overlap(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])))\n",
    "        print(\"overlap after iteration: \" + str(get_overlap(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])))\n",
    "\n",
    "    first_clean_overlap_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])\n",
    "    final_clean_overlap_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])\n",
    "\n",
    "    \n",
    "    if verbose: print(\"-------------Testing--2----------------\")\n",
    "    if verbose: print(\"Testing noise percentage = \"+str(test_noise_frac_s))\n",
    "    \n",
    "    sbook_std = np.std(sbook.flatten())\n",
    "    cts_corrupt = true_data + test_noise_frac_s*sbook_std*np.random.normal(0,1,true_data.shape)\n",
    "    ## mean_s_value = np.average(np.linalg.norm(sbook,axis=1))\n",
    "    ## cts_corrupt = true_data + test_noise_frac_s*mean_s_value*np.random.normal(0,1,true_data.shape)/np.sqrt(Ns)\n",
    "\n",
    "    encoded_data = autoencoder.encoder(cts_corrupt[:Npatts]).numpy()\n",
    "    decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "    decoded_data1 = np.copy(decoded_data)\n",
    "    iters=50\n",
    "    for _ in range(iters):\n",
    "      encoded_data = autoencoder.encoder(decoded_data).numpy()\n",
    "      decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "    \n",
    "    if verbose:\n",
    "      print(\"Np = \"+str(Np))\n",
    "      print(\"Ns = \"+str(Ns))\n",
    "      print(\"Ng = \"+str(Ng))\n",
    "      print(\"Npatts = \"+str(Npatts))\n",
    "\n",
    "\n",
    "      print(\"Initial overlap: \" + str(get_overlap(true_data[:,:data_shape], cts_corrupt[:Npatts,:data_shape])))\n",
    "      print(\"overlap after one timestep: \" + str(get_overlap(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])))\n",
    "      print(\"overlap after iteration: \" + str(get_overlap(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])))\n",
    "\n",
    "    first_noisy_overlap_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])\n",
    "    final_noisy_overlap_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])\n",
    "\n",
    "    \n",
    "    if verbose: print(\"-------------Testing--3----------------\")\n",
    "    if verbose: print(\"Testing noise percentage = \"+str(test_noise_frac_p))\n",
    "    cts_corrupt = corrupt_p(true_data,p=0,booktype='cts')\n",
    "\n",
    "    encoded_data = autoencoder.encoder(cts_corrupt[:Npatts]).numpy()\n",
    "    encoded_data = encoded_data + test_noise_frac_p*mean_p_val*np.random.normal(0,1,encoded_data.shape)/np.sqrt(Np)\n",
    "    decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "    decoded_data1 = np.copy(decoded_data)\n",
    "    iters=50\n",
    "    for _ in range(iters):\n",
    "      encoded_data = autoencoder.encoder(decoded_data).numpy()\n",
    "      decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "    \n",
    "    if verbose:\n",
    "      print(\"Np = \"+str(Np))\n",
    "      print(\"Ns = \"+str(Ns))\n",
    "      print(\"Ng = \"+str(Ng))\n",
    "      print(\"Npatts = \"+str(Npatts))\n",
    "\n",
    "\n",
    "      print(\"Initial error: \" + str(get_mse(true_data[:,:data_shape], cts_corrupt[:Npatts,:data_shape])))\n",
    "      print(\"error after one timestep: \" + str(get_mse(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])))\n",
    "      print(\"error after iteration: \" + str(get_mse(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])))\n",
    "\n",
    "      print(\"Initial overlap: \" + str(get_overlap(true_data[:,:data_shape], cts_corrupt[:Npatts,:data_shape])))\n",
    "      print(\"overlap after one timestep: \" + str(get_overlap(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])))\n",
    "      print(\"overlap after iteration: \" + str(get_overlap(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])))\n",
    "\n",
    "    first_noisy_p_overlap_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])\n",
    "    final_noisy_p_overlap_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plotting/saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "TxesSTtI2EGo",
    "outputId": "40b86cfa-649f-435c-aa34-fd17b5566e78"
   },
   "outputs": [],
   "source": [
    "# plt.plot(Npatts_list,np.average(first_noisy_overlap_list,axis=1), label='noisy, single')\n",
    "# plt.plot(Npatts_list,np.average(final_noisy_overlap_list,axis=1), label='noisy, final')\n",
    "# plt.plot(Npatts_list,np.average(first_clean_overlap_list,axis=1), label='clean, single')\n",
    "# plt.plot(Npatts_list,np.average(final_clean_overlap_list,axis=1), label='clean, final')\n",
    "plt.errorbar(Npatts_list,np.average(first_noisy_p_overlap_list,axis=1),yerr=np.std(first_noisy_overlap_list,axis=1), label='noisy p, single')\n",
    "plt.errorbar(Npatts_list,np.average(final_noisy_p_overlap_list,axis=1),yerr=np.std(final_noisy_overlap_list,axis=1), label='noisy p, final')\n",
    "\n",
    "plt.errorbar(Npatts_list,np.average(first_noisy_overlap_list,axis=1),yerr=np.std(first_noisy_overlap_list,axis=1), label='noisy s, single')\n",
    "plt.errorbar(Npatts_list,np.average(final_noisy_overlap_list,axis=1),yerr=np.std(final_noisy_overlap_list,axis=1), label='noisy s, final')\n",
    "\n",
    "plt.errorbar(Npatts_list,np.average(first_clean_overlap_list,axis=1),yerr=np.std(first_clean_overlap_list,axis=1), label='clean, single')\n",
    "plt.errorbar(Npatts_list,np.average(final_clean_overlap_list,axis=1),yerr=np.std(final_clean_overlap_list,axis=1), label='clean, final')\n",
    "plt.xlabel('Number of patterns')\n",
    "plt.ylabel('Cosine similarity')\n",
    "plt.title('noise val = '+str(test_noise_frac_s))\n",
    "plt.xscale('log');\n",
    "plt.yscale('log');\n",
    "plt.legend()\n",
    "# plt.savefig(\"overlap_list_Np_\"+str(Np)+\"_Ns_\"+str(Ns)+\"_Ng_\"+str(Ng)+\"_nruns_\"+str(nruns)+\"_miniimagenet_BW_perm_2.0_noisy_s_p.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "AhIlbAFISbH_",
    "outputId": "b3f4daf0-0172-4d09-9d92-cbbc0c386cd5"
   },
   "outputs": [],
   "source": [
    "np.save(\"first_clean_overlap_list_Np_\"+str(Np)+\"_Ns_\"+str(Ns)+\"_Ng_\"+str(Ng)+\"_nruns_\"+str(nruns)+\"_miniimagenet_BW_perm_2.0_noisy_s_p.npy\", first_clean_overlap_list)\n",
    "np.save(\"final_clean_overlap_list_Np_\"+str(Np)+\"_Ns_\"+str(Ns)+\"_Ng_\"+str(Ng)+\"_nruns_\"+str(nruns)+\"_miniimagenet_BW_perm_2.0_noisy_s_p.npy\", final_clean_overlap_list)\n",
    "np.save(\"first_noisy_overlap_list_Np_\"+str(Np)+\"_Ns_\"+str(Ns)+\"_Ng_\"+str(Ng)+\"_nruns_\"+str(nruns)+\"_miniimagenet_BW_perm_2.0_noisy_s_p.npy\", first_noisy_overlap_list)\n",
    "np.save(\"final_noisy_overlap_list_Np_\"+str(Np)+\"_Ns_\"+str(Ns)+\"_Ng_\"+str(Ng)+\"_nruns_\"+str(nruns)+\"_miniimagenet_BW_perm_2.0_noisy_s_p.npy\", final_noisy_overlap_list)\n",
    "np.save(\"first_noisy_p_overlap_list_Np_\"+str(Np)+\"_Ns_\"+str(Ns)+\"_Ng_\"+str(Ng)+\"_nruns_\"+str(nruns)+\"_miniimagenet_BW_perm_2.0_noisy_s_p.npy\", first_noisy_p_overlap_list)\n",
    "np.save(\"final_noisy_p_overlap_list_Np_\"+str(Np)+\"_Ns_\"+str(Ns)+\"_Ng_\"+str(Ng)+\"_nruns_\"+str(nruns)+\"_miniimagenet_BW_perm_2.0_noisy_s_p.npy\", final_noisy_p_overlap_list)\n",
    "\n",
    "\n",
    "np.save(\"Npatts_list_miniimagenet_BW_perm_2.0_noisy_s_p.npy\", Npatts_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "S8EmtlnDfcC0",
    "outputId": "10bc79e1-2e04-40fc-9b7d-529ab075e893"
   },
   "outputs": [],
   "source": [
    "np.save(\"first_clean_overlap_list_Np_\"+str(Np)+\"_Ns_\"+str(Ns)+\"_Ng_\"+str(Ng)+\"_nruns_\"+str(nruns)+\"_random_binary_0.1_sflip_1.0_pnoise.npy\", first_clean_overlap_list)\n",
    "np.save(\"final_clean_overlap_list_Np_\"+str(Np)+\"_Ns_\"+str(Ns)+\"_Ng_\"+str(Ng)+\"_nruns_\"+str(nruns)+\"_random_binary_0.1_sflip_1.0_pnoise.npy\", final_clean_overlap_list)\n",
    "np.save(\"first_noisy_overlap_list_Np_\"+str(Np)+\"_Ns_\"+str(Ns)+\"_Ng_\"+str(Ng)+\"_nruns_\"+str(nruns)+\"_random_binary_0.1_sflip_1.0_pnoise.npy\", first_noisy_overlap_list)\n",
    "np.save(\"final_noisy_overlap_list_Np_\"+str(Np)+\"_Ns_\"+str(Ns)+\"_Ng_\"+str(Ng)+\"_nruns_\"+str(nruns)+\"_random_binary_0.1_sflip_1.0_pnoise.npy\", final_noisy_overlap_list)\n",
    "np.save(\"first_noisy_p_overlap_list_Np_\"+str(Np)+\"_Ns_\"+str(Ns)+\"_Ng_\"+str(Ng)+\"_nruns_\"+str(nruns)+\"_random_binary_0.1_sflip_1.0_pnoise.npy\", first_noisy_p_overlap_list)\n",
    "np.save(\"final_noisy_p_overlap_list_Np_\"+str(Np)+\"_Ns_\"+str(Ns)+\"_Ng_\"+str(Ng)+\"_nruns_\"+str(nruns)+\"_random_binary_0.1_sflip_1.0_pnoise.npy\", final_noisy_p_overlap_list)\n",
    "\n",
    "\n",
    "np.save(\"Npatts_list_random_binary_0.1_sflip_1.0_pnoise.npy\", Npatts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "RwwQcPj9mpmR",
    "outputId": "37950f4f-388a-4429-cc89-7221207fd159"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,8))\n",
    "plt.plot(Npatts_list,np.average(first_noisy_overlap_list,axis=1), label='noisy, single')\n",
    "plt.plot(Npatts_list,np.average(final_noisy_overlap_list,axis=1), label='noisy, final')\n",
    "plt.plot(Npatts_list,np.average(first_clean_overlap_list,axis=1), label='clean, single')\n",
    "plt.plot(Npatts_list,np.average(final_clean_overlap_list,axis=1), label='clean, final')\n",
    "# plt.errorbar(Npatts_list,np.average(first_noisy_overlap_list,axis=1),yerr=np.std(first_noisy_overlap_list,axis=1), label='noisy, single')\n",
    "# plt.errorbar(Npatts_list,np.average(final_noisy_overlap_list,axis=1),yerr=np.std(final_noisy_overlap_list,axis=1), label='noisy, final')\n",
    "# plt.errorbar(Npatts_list,np.average(first_clean_overlap_list,axis=1),yerr=np.std(first_clean_overlap_list,axis=1), label='clean, single')\n",
    "# plt.errorbar(Npatts_list,np.average(final_clean_overlap_list,axis=1),yerr=np.std(final_clean_overlap_list,axis=1), label='clean, final')\n",
    "plt.xlabel('Number of patterns')\n",
    "plt.ylabel('Overlap')\n",
    "plt.legend()\n",
    "plt.savefig(\"overlap_list_Np_\"+str(Np)+\"_Ns_\"+str(Ns)+\"_Ng_\"+str(Ng)+\"_nruns_\"+str(nruns)+\".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNCLcqdfRiEo"
   },
   "outputs": [],
   "source": [
    "Ng=18\n",
    "Np=300\n",
    "Ns=816\n",
    "first_clean_overlap_list = np.load(\"first_clean_overlap_list_Np_\"+str(Np)+\"_Ns_\"+str(Ns)+\"_Ng_\"+str(Ng)+\".npy\")\n",
    "final_clean_overlap_list = np.load(\"final_clean_overlap_list_Np_\"+str(Np)+\"_Ns_\"+str(Ns)+\"_Ng_\"+str(Ng)+\".npy\")\n",
    "first_noisy_overlap_list = np.load(\"first_noisy_overlap_list_Np_\"+str(Np)+\"_Ns_\"+str(Ns)+\"_Ng_\"+str(Ng)+\".npy\")\n",
    "final_noisy_overlap_list = np.load(\"final_noisy_overlap_list_Np_\"+str(Np)+\"_Ns_\"+str(Ns)+\"_Ng_\"+str(Ng)+\".npy\")\n",
    "Npatts_list = np.load(\"Npatts_list.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoIcytYCfSdC",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## iterative increase (not used in paper, qualitatively similar results to above)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JirZvnhAfU48",
    "outputId": "33fd85de-a955-40a5-d99b-040c190e099c"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "Npatts_list = np.arange(1,3601,50)\n",
    "dNpatts_list = np.diff(Npatts_list)\n",
    "np.insert(dNpatts_list,0,Npatts_list[0])\n",
    "# Npatts_list = np.logspace(2,np.log10(3600),25).astype('int')[::-1]\n",
    "nruns = 3\n",
    "first_clean_overlap_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "final_clean_overlap_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "# first_clean_overlap_not_norm_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "# final_clean_overlap_not_norm_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "# first_clean_l1_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "# final_clean_l1_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "# first_clean_mse_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "# final_clean_mse_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "\n",
    "first_noisy_overlap_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "final_noisy_overlap_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "\n",
    "first_noisy_p_overlap_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "final_noisy_p_overlap_list = np.zeros((Npatts_list.shape[0],nruns))\n",
    "\n",
    "verbose=False\n",
    "\n",
    "for nidx in (range(nruns)):\n",
    "  Npatts = 0\n",
    "  Ng=9+16+25\n",
    "  Np=400\n",
    "  Ns=3600#84*84*3\n",
    "  # Npatts=200\n",
    "  data_shape = Ns\n",
    "  test_noise_frac=0.0\n",
    "  test_noise_frac_s=1.\n",
    "  test_noise_frac_p=1.\n",
    "\n",
    "  sbook = np.sign(np.random.uniform(size=(max(Npatts_list),Ns))-0.5)\n",
    "\n",
    "  # sbook = data_features - np.mean(data_features.flatten())\n",
    "\n",
    "\n",
    "  #sperm = sbook[np.random.permutation(3600)]\n",
    "  # true_data = sperm[:Npatts]   #data_features[:Npatts]   #double_train_data_cts  #train_data_cts   #noise_aug_train_data  #sbook # bin_train[:Npatts] #\n",
    "\n",
    "  class Autoencoder(Model):\n",
    "    def __init__(self, Np,Ng,Ns):\n",
    "      super(Autoencoder, self).__init__()\n",
    "      self.Np = Np\n",
    "      self.Ns = Ns\n",
    "      self.Ng = Ng\n",
    "      self.encoder = tf.keras.Sequential([\n",
    "        layers.Dense(Np, activation='tanh'),\n",
    "        # layers.Dense(Ng, activation='tanh'),\n",
    "      ])\n",
    "      self.decoder = tf.keras.Sequential([\n",
    "        layers.Dense(Ng, activation='tanh'),\n",
    "        layers.Dense(Np, activation='tanh'),\n",
    "        layers.Dense(Ns, activation='tanh')\n",
    "      ])\n",
    "\n",
    "    def call(self, x):\n",
    "      encoded = self.encoder(x)\n",
    "      decoded = self.decoder(encoded)\n",
    "      return decoded\n",
    "\n",
    "  autoencoder = Autoencoder(Np,Ng,Ns)\n",
    "\n",
    "  autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\n",
    "  for idx,dNpatts in enumerate(tqdm.tqdm(dNpatts_list)):\n",
    "    if verbose:\n",
    "      print(\"xxxxx nidx = \"+str(nidx))\n",
    "      print(\"xxxxx idx = \"+str(idx))\n",
    "      print(\"xxxxx Npatts = \"+str(Npatts))\n",
    "\n",
    "    train_data = sbook[Npatts:Npatts+dNpatts]\n",
    "    autoencoder.fit(train_data, train_data, epochs=3000, verbose=0);\n",
    "    Npatts = Npatts + dNpatts\n",
    "\n",
    "\n",
    "    # if verbose: print(\"-------------Testing--1----------------\")\n",
    "    # if verbose: print(\"Testing noise percentage = \"+str(test_noise_frac))\n",
    "    true_data = sbook[:Npatts]\n",
    "    cts_corrupt = corrupt_p(true_data,p=0,booktype='cts')\n",
    "\n",
    "    encoded_data = autoencoder.encoder(cts_corrupt[:Npatts]).numpy()\n",
    "    decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "    #decoded_data1 = binarize(np.copy(decoded_data),bin_type='-11')\n",
    "    decoded_data1 = np.copy(decoded_data)\n",
    "    iters=50\n",
    "    for _ in range(iters):\n",
    "      encoded_data = autoencoder.encoder(decoded_data).numpy()\n",
    "      decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "    #decoded_data = binarize(decoded_data,bin_type='-11')\n",
    "    mean_p_val = np.average(np.linalg.norm(encoded_data,axis=1))\n",
    "\n",
    "    if verbose:\n",
    "      print(\"Npatts = \"+str(Npatts))\n",
    "      # print(\"Initial error: \" + str(get_mse(true_data[:,:data_shape], cts_corrupt[:Npatts,:data_shape])))\n",
    "      # print(\"error after one timestep: \" + str(get_mse(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])))\n",
    "      # print(\"error after iteration: \" + str(get_mse(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])))\n",
    "\n",
    "      # print(\"Initial overlap: \" + str(get_overlap(true_data[:,:data_shape], cts_corrupt[:Npatts,:data_shape])))\n",
    "      print(\"overlap after one timestep: \" + str(get_overlap(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])))\n",
    "      print(\"overlap after iteration: \" + str(get_overlap(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])))\n",
    "\n",
    "    first_clean_overlap_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])\n",
    "    final_clean_overlap_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])\n",
    "\n",
    "    # first_clean_overlap_not_norm_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape],normalized=False)\n",
    "    # final_clean_overlap_not_norm_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape],normalized=False)\n",
    "\n",
    "    # first_clean_l1_list[idx,nidx]=get_error(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])\n",
    "    # final_clean_l1_list[idx,nidx]=get_error(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])\n",
    "\n",
    "    # first_clean_mse_list[idx,nidx]=get_mse(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])\n",
    "    # final_clean_mse_list[idx,nidx]=get_mse(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])\n",
    "\n",
    "    if verbose: print(\"-------------Testing--2----------------\")\n",
    "    if verbose: print(\"Testing noise percentage = \"+str(test_noise_frac_s))\n",
    "    # cts_corrupt = corrupt_p(true_data,p=test_noise_frac_s,booktype='cts')\n",
    "\n",
    "    mean_s_value = np.average(np.linalg.norm(true_data,axis=1))\n",
    "    cts_corrupt = true_data + test_noise_frac_s*mean_s_value*np.random.normal(0,1,true_data.shape)/np.sqrt(Ns)\n",
    "\n",
    "    encoded_data = autoencoder.encoder(cts_corrupt[:Npatts]).numpy()\n",
    "    decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "    #decoded_data1 = binarize(np.copy(decoded_data),bin_type='-11')\n",
    "    decoded_data1 = np.copy(decoded_data)\n",
    "    iters=50\n",
    "    for _ in range(iters):\n",
    "      encoded_data = autoencoder.encoder(decoded_data).numpy()\n",
    "      decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "    #decoded_data = binarize(decoded_data,bin_type='-11')\n",
    "\n",
    "    if verbose:\n",
    "      print(\"Np = \"+str(Np))\n",
    "      print(\"Ns = \"+str(Ns))\n",
    "      print(\"Ng = \"+str(Ng))\n",
    "      print(\"Npatts = \"+str(Npatts))\n",
    "\n",
    "\n",
    "      print(\"Initial error: \" + str(get_mse(true_data[:,:data_shape], cts_corrupt[:Npatts,:data_shape])))\n",
    "      print(\"error after one timestep: \" + str(get_mse(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])))\n",
    "      print(\"error after iteration: \" + str(get_mse(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])))\n",
    "\n",
    "      print(\"Initial overlap: \" + str(get_overlap(true_data[:,:data_shape], cts_corrupt[:Npatts,:data_shape])))\n",
    "      print(\"overlap after one timestep: \" + str(get_overlap(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])))\n",
    "      print(\"overlap after iteration: \" + str(get_overlap(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])))\n",
    "\n",
    "    first_noisy_overlap_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])\n",
    "    final_noisy_overlap_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])\n",
    "\n",
    "    # first_noisy_overlap_not_norm_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape],normalized=False)\n",
    "    # final_noisy_overlap_not_norm_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape],normalized=False)\n",
    "\n",
    "    # first_noisy_l1_list[idx,nidx]=get_error(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])\n",
    "    # final_noisy_l1_list[idx,nidx]=get_error(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])\n",
    "\n",
    "    # first_noisy_mse_list[idx,nidx]=get_mse(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])\n",
    "    # final_noisy_mse_list[idx,nidx]=get_mse(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])\n",
    "\n",
    "    if verbose: print(\"-------------Testing--3----------------\")\n",
    "    if verbose: print(\"Testing noise percentage = \"+str(test_noise_frac_p))\n",
    "    cts_corrupt = corrupt_p(true_data,p=0,booktype='cts')\n",
    "\n",
    "    encoded_data = autoencoder.encoder(cts_corrupt[:Npatts]).numpy()\n",
    "    encoded_data = encoded_data + test_noise_frac_p*mean_p_val*np.random.normal(0,1,encoded_data.shape)/np.sqrt(Np)\n",
    "    decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "    #decoded_data1 = binarize(np.copy(decoded_data),bin_type='-11')\n",
    "    decoded_data1 = np.copy(decoded_data)\n",
    "    iters=50\n",
    "    for _ in range(iters):\n",
    "      encoded_data = autoencoder.encoder(decoded_data).numpy()\n",
    "      decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "    #decoded_data = binarize(decoded_data,bin_type='-11')\n",
    "\n",
    "    if verbose:\n",
    "      print(\"Np = \"+str(Np))\n",
    "      print(\"Ns = \"+str(Ns))\n",
    "      print(\"Ng = \"+str(Ng))\n",
    "      print(\"Npatts = \"+str(Npatts))\n",
    "\n",
    "\n",
    "      print(\"Initial error: \" + str(get_mse(true_data[:,:data_shape], cts_corrupt[:Npatts,:data_shape])))\n",
    "      print(\"error after one timestep: \" + str(get_mse(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])))\n",
    "      print(\"error after iteration: \" + str(get_mse(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])))\n",
    "\n",
    "      print(\"Initial overlap: \" + str(get_overlap(true_data[:,:data_shape], cts_corrupt[:Npatts,:data_shape])))\n",
    "      print(\"overlap after one timestep: \" + str(get_overlap(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])))\n",
    "      print(\"overlap after iteration: \" + str(get_overlap(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])))\n",
    "\n",
    "    first_noisy_p_overlap_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])\n",
    "    final_noisy_p_overlap_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])\n",
    "\n",
    "    # first_noisy_overlap_not_norm_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape],normalized=False)\n",
    "    # final_noisy_overlap_not_norm_list[idx,nidx]=get_overlap(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape],normalized=False)\n",
    "\n",
    "    # first_noisy_l1_list[idx,nidx]=get_error(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])\n",
    "    # final_noisy_l1_list[idx,nidx]=get_error(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])\n",
    "\n",
    "    # first_noisy_mse_list[idx,nidx]=get_mse(true_data[:Npatts,:data_shape],decoded_data1[:Npatts,:data_shape])\n",
    "    # final_noisy_mse_list[idx,nidx]=get_mse(true_data[:Npatts,:data_shape],decoded_data[:Npatts,:data_shape])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "PmoCZ0SAjYnh",
    "outputId": "c6b4329f-df24-4997-9bc1-cb36d4674565"
   },
   "outputs": [],
   "source": [
    "# plt.plot(Npatts_list,np.average(first_noisy_overlap_list,axis=1), label='noisy, single')\n",
    "# plt.plot(Npatts_list,np.average(final_noisy_overlap_list,axis=1), label='noisy, final')\n",
    "# plt.plot(Npatts_list,np.average(first_clean_overlap_list,axis=1), label='clean, single')\n",
    "# plt.plot(Npatts_list,np.average(final_clean_overlap_list,axis=1), label='clean, final')\n",
    "plt.errorbar(Npatts_list,np.average(first_noisy_p_overlap_list,axis=1),yerr=np.std(first_noisy_overlap_list,axis=1), label='noisy p, single')\n",
    "plt.errorbar(Npatts_list,np.average(final_noisy_p_overlap_list,axis=1),yerr=np.std(final_noisy_overlap_list,axis=1), label='noisy p, final')\n",
    "\n",
    "plt.errorbar(Npatts_list,np.average(first_noisy_overlap_list,axis=1),yerr=np.std(first_noisy_overlap_list,axis=1), label='noisy s, single')\n",
    "plt.errorbar(Npatts_list,np.average(final_noisy_overlap_list,axis=1),yerr=np.std(final_noisy_overlap_list,axis=1), label='noisy s, final')\n",
    "plt.errorbar(Npatts_list,np.average(first_clean_overlap_list,axis=1),yerr=np.std(first_clean_overlap_list,axis=1), label='clean, single')\n",
    "plt.errorbar(Npatts_list,np.average(final_clean_overlap_list,axis=1),yerr=np.std(final_clean_overlap_list,axis=1), label='clean, final')\n",
    "plt.xlabel('Number of patterns')\n",
    "plt.ylabel('Cosine similarity')\n",
    "plt.title('noise val = '+str(test_noise_frac_s))\n",
    "# plt.plot([1,10],[1,0.1],'k--')\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbook_std = np.std(sbook.flatten())\n",
    "# # scorrupt_test = corrupt_p(sbook,p=0.2,booktype='cts')\n",
    "# scorrupt_test = sbook + np.random.normal(0,1,sbook.shape)*1*sbook_std\n",
    "# #scorrupt_test = 0.5*(scorrupt_test+1)\n",
    "\n",
    "# sbookf = sbook[:Npatts].reshape((Npatts,60,60))\n",
    "# scorrupt_test = np.copy(sbookf)\n",
    "# scorrupt_test[:,:,20:40] = np.random.normal(0,0.1,(Npatts,60,20))\n",
    "# scorrupt_test = scorrupt_test.reshape((Npatts,3600))\n",
    "\n",
    "scorrupt_test = corrupt_p(sbook,p=0.1,booktype='-11')\n",
    "\n",
    "encoded_data = autoencoder.encoder(scorrupt_test[:Npatts]).numpy()\n",
    "\n",
    "# encoded_data = encoded_data + 0.5*mean_p_val*np.random.normal(0,1,encoded_data.shape)/np.sqrt(Np)\n",
    "decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "decoded_data1 = np.copy(decoded_data)\n",
    "iters=100\n",
    "ovp_ite = np.zeros(iters)\n",
    "for i in tqdm(range(iters)):\n",
    "    encoded_data = autoencoder.encoder(decoded_data).numpy()\n",
    "    decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
    "    ovp_ite[i] = get_overlap(sbook[:Npatts],decoded_data)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "bioinfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
